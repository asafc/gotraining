---
title: "Working with the Kubernetes API"
---

In the previous chapter, we built a basic Kubernetes operator using controller-runtime,
which provides a high-level abstraction for interacting with the Kubernetes API.

In this chapter, we'll dive deeper into how to work effectively with the Kubernetes API through client-go, explore various patterns for managing resources, and understand watches and informers in more detail.

## Client-go: The Kubernetes API Client Library

[client-go](https://github.com/kubernetes/client-go) is the official Go client library for Kubernetes. It provides various ways to interact with the Kubernetes API server:

1. **Clientset**: Type-safe clients for built-in Kubernetes resources
2. **Dynamic Client**: For working with resources without compile-time types
3. **REST Client**: Low-level client for making HTTP requests to the API server
4. **Informers/Listers**: For efficient watch and list operations with caching

### Setting up client-go

Here's how to set up a client-go clientset:

```go
import (
    "k8s.io/client-go/kubernetes"
    "k8s.io/client-go/rest"
    "k8s.io/client-go/tools/clientcmd"
)

// In-cluster configuration (for code running inside Kubernetes)
func getInClusterClientset() (*kubernetes.Clientset, error) {
    config, err := rest.InClusterConfig()
    if err != nil {
        return nil, err
    }
    return kubernetes.NewForConfig(config)
}

// Out-of-cluster configuration (for local development)
func getOutOfClusterClientset(kubeconfig string) (*kubernetes.Clientset, error) {
    config, err := clientcmd.BuildConfigFromFlags("", kubeconfig)
    if err != nil {
        return nil, err
    }
    return kubernetes.NewForConfig(config)
}
```

## CRUD Operations with Client-go

Let's look at how to perform basic CRUD (Create, Read, Update, Delete) operations using client-go.

### Creating Resources

```go
import (
    "context"
    appsv1 "k8s.io/api/apps/v1"
    corev1 "k8s.io/api/core/v1"
    metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

// Create a deployment
func createDeployment(clientset *kubernetes.Clientset, namespace string) (*appsv1.Deployment, error) {
    deploymentsClient := clientset.AppsV1().Deployments(namespace)
    
    deployment := &appsv1.Deployment{
        ObjectMeta: metav1.ObjectMeta{
            Name: "example-deployment",
        },
        Spec: appsv1.DeploymentSpec{
            Replicas: int32Ptr(3),
            Selector: &metav1.LabelSelector{
                MatchLabels: map[string]string{
                    "app": "example",
                },
            },
            Template: corev1.PodTemplateSpec{
                ObjectMeta: metav1.ObjectMeta{
                    Labels: map[string]string{
                        "app": "example",
                    },
                },
                Spec: corev1.PodSpec{
                    Containers: []corev1.Container{
                        {
                            Name:  "web",
                            Image: "nginx:1.19",
                            Ports: []corev1.ContainerPort{
                                {
                                    Name:          "http",
                                    Protocol:      corev1.ProtocolTCP,
                                    ContainerPort: 80,
                                },
                            },
                        },
                    },
                },
            },
        },
    }
    
    // Create Deployment
    return deploymentsClient.Create(context.TODO(), deployment, metav1.CreateOptions{})
}

func int32Ptr(i int32) *int32 { return &i }
```

### Reading Resources

```go
// Get a specific deployment
func getDeployment(clientset *kubernetes.Clientset, namespace, name string) (*appsv1.Deployment, error) {
    return clientset.AppsV1().Deployments(namespace).Get(context.TODO(), name, metav1.GetOptions{})
}

// List deployments with label selector
func listDeployments(clientset *kubernetes.Clientset, namespace string, labelSelector string) (*appsv1.DeploymentList, error) {
    return clientset.AppsV1().Deployments(namespace).List(context.TODO(), metav1.ListOptions{
        LabelSelector: labelSelector,
    })
}
```

### Updating Resources

There are two main approaches to updating resources:

1. **Update**: Full update of a resource, but can fail if the resource was modified in the meantime
2. **Patch**: Apply partial changes to a resource

```go
// Update a deployment
func updateDeployment(clientset *kubernetes.Clientset, namespace string, deployment *appsv1.Deployment) (*appsv1.Deployment, error) {
    return clientset.AppsV1().Deployments(namespace).Update(context.TODO(), deployment, metav1.UpdateOptions{})
}

// Patch a deployment (using strategic merge patch)
func patchDeployment(clientset *kubernetes.Clientset, namespace, name string) (*appsv1.Deployment, error) {
    patchData := []byte(`{"spec":{"replicas":5}}`)
    return clientset.AppsV1().Deployments(namespace).Patch(
        context.TODO(),
        name,
        types.StrategicMergePatchType,
        patchData,
        metav1.PatchOptions{},
    )
}
```

#### Update vs. Patch

- **Update** requires fetching the resource first, modifying it, and then sending the full resource back
- **Patch** allows specifying only the changes to apply without fetching the resource first
- Patch is generally safer in concurrent environments as it reduces the chance of conflicts

### Deleting Resources

```go
// Delete a deployment
func deleteDeployment(clientset *kubernetes.Clientset, namespace, name string) error {
    return clientset.AppsV1().Deployments(namespace).Delete(
        context.TODO(),
        name,
        metav1.DeleteOptions{},
    )
}

// Delete deployments with label selector
func deleteDeploymentsByLabel(clientset *kubernetes.Clientset, namespace, labelSelector string) error {
    return clientset.AppsV1().Deployments(namespace).DeleteCollection(
        context.TODO(),
        metav1.DeleteOptions{},
        metav1.ListOptions{
            LabelSelector: labelSelector,
        },
    )
}
```

## Owner References and Garbage Collection

Kubernetes uses owner references to manage the lifecycle of dependent objects. When you create resources that should be owned by your custom resource, you should set owner references to enable automatic garbage collection when the parent resource is deleted.

```go
import (
    metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

// Set owner reference on a resource
func setOwnerReference(owner, dependent metav1.Object, scheme *runtime.Scheme) error {
    // Set controller reference (used in operators)
    return controllerutil.SetControllerReference(owner, dependent, scheme)
    
    // Alternatively, manually set owner reference
    /*
    ownerRef := metav1.OwnerReference{
        APIVersion: apiVersion,
        Kind:       kind,
        Name:       owner.GetName(),
        UID:        owner.GetUID(),
        Controller: pointer.BoolPtr(true),
    }
    dependent.SetOwnerReferences(append(dependent.GetOwnerReferences(), ownerRef))
    */
}
```

### Garbage Collection Policies

Kubernetes supports three garbage collection policies:

1. **Orphan**: Dependent objects are orphaned when the owner is deleted
2. **Background**: Dependent objects are deleted in the background after the owner is deleted
3. **Foreground**: Owner deletion is blocked until all dependent objects are deleted

```go
// Delete with foreground garbage collection
func deleteForeground(clientset *kubernetes.Clientset, namespace, name string) error {
    foregroundDelete := metav1.DeletePropagationForeground
    return clientset.AppsV1().Deployments(namespace).Delete(
        context.TODO(),
        name,
        metav1.DeleteOptions{
            PropagationPolicy: &foregroundDelete,
        },
    )
}
```

## Understanding Watches and Informers

Watches and informers are fundamental to the Kubernetes controller pattern, allowing efficient observation of resource changes.

### Watches

A watch is a long-running HTTP request that streams changes to resources:

```go
func watchDeployments(clientset *kubernetes.Clientset, namespace string) {
    watch, err := clientset.AppsV1().Deployments(namespace).Watch(context.TODO(), metav1.ListOptions{})
    if err != nil {
        panic(err)
    }
    
    for event := range watch.ResultChan() {
        deployment, ok := event.Object.(*appsv1.Deployment)
        if !ok {
            continue
        }
        
        fmt.Printf("Event: %s Deployment: %s\n", event.Type, deployment.Name)
    }
}
```

Watches are efficient but have limitations:
- They don't handle connection failures automatically
- They don't cache resources
- They require resyncs to ensure no events are missed

### Informers and Listers

Informers solve these limitations by providing:

1. Automatic reconnection for watches
2. Local caching of resources
3. Resource indexing
4. Event handlers for resource changes

Here's a simple example of using informers:

```go
import (
    "k8s.io/client-go/informers"
    "k8s.io/client-go/tools/cache"
)

func setupInformers(clientset *kubernetes.Clientset, namespace string) {
    // Create a shared informer factory
    factory := informers.NewSharedInformerFactoryWithOptions(
        clientset,
        10*time.Minute, // Resync period
        informers.WithNamespace(namespace),
    )
    
    // Create a deployment informer
    deploymentInformer := factory.Apps().V1().Deployments().Informer()
    
    // Add event handlers
    deploymentInformer.AddEventHandler(cache.ResourceEventHandlerFuncs{
        AddFunc: func(obj interface{}) {
            deployment := obj.(*appsv1.Deployment)
            fmt.Printf("Deployment added: %s\n", deployment.Name)
        },
        UpdateFunc: func(oldObj, newObj interface{}) {
            oldDeployment := oldObj.(*appsv1.Deployment)
            newDeployment := newObj.(*appsv1.Deployment)
            fmt.Printf("Deployment updated: %s, old replicas: %d, new replicas: %d\n",
                newDeployment.Name,
                *oldDeployment.Spec.Replicas,
                *newDeployment.Spec.Replicas,
            )
        },
        DeleteFunc: func(obj interface{}) {
            deployment := obj.(*appsv1.Deployment)
            fmt.Printf("Deployment deleted: %s\n", deployment.Name)
        },
    })
    
    // Start the informers
    stopCh := make(chan struct{})
    factory.Start(stopCh)
    
    // Wait for the initial sync
    factory.WaitForCacheSync(stopCh)
    
    // Use a lister to access cached objects
    deploymentLister := factory.Apps().V1().Deployments().Lister()
    
    // This will use the cache, not hitting the API server
    cachedDeployment, err := deploymentLister.Deployments(namespace).Get("example-deployment")
    if err != nil {
        panic(err)
    }
    
    fmt.Printf("Cached deployment: %s\n", cachedDeployment.Name)
}
```

## Informers in Practice with controller-runtime

The controller-runtime library, which we used in the previous chapter, abstracts away much of the complexity of working with informers. Under the hood, it sets up watches and informers for the resources you specify.

Let's look at how controller-runtime sets up watchers:

```go
func setupController(mgr ctrl.Manager) error {
    return ctrl.NewControllerManagedBy(mgr).
        // Watch WebService resources
        For(&appsv1alpha1.WebService{}).
        // Watch Deployments owned by WebService resources
        Owns(&appsv1.Deployment{}).
        // Watch Services owned by WebService resources 
        Owns(&corev1.Service{}).
        // Custom watches for resources not directly owned
        Watches(
            &source.Kind{Type: &corev1.ConfigMap{}},
            handler.EnqueueRequestsFromMapFunc(func(obj client.Object) []reconcile.Request {
                // Map ConfigMap to owner WebService
                // Return the reconcile Request for the owner
                return []reconcile.Request{
                    {
                        NamespacedName: types.NamespacedName{
                            Name:      getOwnerFromLabels(obj),
                            Namespace: obj.GetNamespace(),
                        },
                    },
                }
            }),
        ).
        Complete(&WebServiceReconciler{Client: mgr.GetClient()})
}
```

### Using Field Selectors and Label Selectors

Field and label selectors can make your watches more efficient by filtering the resources that trigger reconciliations:

```go
// Set up a controller with a label selector
func setupControllerWithSelector(mgr ctrl.Manager) error {
    // Create a predicate that filters by label
    labelPredicate := predicate.NewPredicateFuncs(func(object client.Object) bool {
        return object.GetLabels()["app"] == "example"
    })
    
    return ctrl.NewControllerManagedBy(mgr).
        For(&appsv1alpha1.WebService{}, builder.WithPredicates(labelPredicate)).
        Complete(&WebServiceReconciler{Client: mgr.GetClient()})
}
```

## Caching and Performance Considerations

When working with the Kubernetes API, it's important to be mindful of performance considerations:

1. **Use caching**: Informers and listers cache resources locally, reducing API server load
2. **Batch operations**: Use label selectors to fetch multiple resources in a single request
3. **Use server-side filtering**: Use field and label selectors to filter resources on the server side
4. **Use pagination**: For large result sets, use pagination (continue tokens) to avoid timeouts
5. **Avoid unnecessary watches**: Only watch resources that are relevant to your controller
6. **Optimize resync periods**: Balance between responsiveness and API server load

## Working with Custom Resources

So far, we've focused on working with built-in Kubernetes resources. Let's now explore how to interact with custom resources that you've defined.

### Client Options for Custom Resources

There are several approaches to working with custom resources:

1. **Generated clientset**: Using code-generator to create typed clients
2. **controller-runtime client**: A generic client that works with any resource type
3. **Dynamic client**: For when you need to work with resources without compile-time types

### Using Generated Clientset

If you've created a custom resource using kubebuilder or operator-sdk, the code generators will create typed clients for your CRDs:

```go
import (
    clientset "github.com/example/webservice-operator/pkg/generated/clientset/versioned"
)

func getWebService(client *clientset.Clientset, namespace, name string) (*appsv1alpha1.WebService, error) {
    return client.AppsV1alpha1().WebServices(namespace).Get(context.TODO(), name, metav1.GetOptions{})
}
```

### Using controller-runtime Client

The controller-runtime client works with any resource type, including custom resources:

```go
import (
    "sigs.k8s.io/controller-runtime/pkg/client"
)

func getWebService(c client.Client, namespace, name string) (*appsv1alpha1.WebService, error) {
    webService := &appsv1alpha1.WebService{}
    err := c.Get(context.TODO(), client.ObjectKey{Namespace: namespace, Name: name}, webService)
    return webService, err
}
```

### Using Dynamic Client

The dynamic client is useful when you need to work with resources without compile-time types:

```go
import (
    "k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
    "k8s.io/apimachinery/pkg/runtime/schema"
    "k8s.io/client-go/dynamic"
)

func getWebServiceWithDynamicClient(dynamicClient dynamic.Interface, namespace, name string) (*unstructured.Unstructured, error) {
    // Define the GVR (Group-Version-Resource)
    gvr := schema.GroupVersionResource{
        Group:    "apps.example.com",
        Version:  "v1alpha1",
        Resource: "webservices",
    }
    
    // Get the resource
    return dynamicClient.Resource(gvr).Namespace(namespace).Get(context.TODO(), name, metav1.GetOptions{})
}
```

## Advanced Patterns

### Server-Side Apply

Server-side apply is a newer feature in Kubernetes that simplifies resource management. It allows you to:

1. Submit partial updates to resources
2. Track field ownership
3. Resolve conflicts automatically based on ownership

```go
import (
    "k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
    metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

func serverSideApply(clientset *kubernetes.Clientset) error {
    // Create the deployment object
    deployment := &appsv1.Deployment{
        TypeMeta: metav1.TypeMeta{
            APIVersion: "apps/v1",
            Kind:       "Deployment",
        },
        ObjectMeta: metav1.ObjectMeta{
            Name:      "example-deployment",
            Namespace: "default",
        },
        Spec: appsv1.DeploymentSpec{
            Replicas: int32Ptr(3),
            Selector: &metav1.LabelSelector{
                MatchLabels: map[string]string{
                    "app": "example",
                },
            },
            Template: corev1.PodTemplateSpec{
                ObjectMeta: metav1.ObjectMeta{
                    Labels: map[string]string{
                        "app": "example",
                    },
                },
                Spec: corev1.PodSpec{
                    Containers: []corev1.Container{
                        {
                            Name:  "web",
                            Image: "nginx:1.19",
                        },
                    },
                },
            },
        },
    }
    
    // Convert to unstructured
    obj, err := runtime.DefaultUnstructuredConverter.ToUnstructured(deployment)
    if err != nil {
        return err
    }
    unstructured := &unstructured.Unstructured{Object: obj}
    
    // Apply the deployment using server-side apply
    _, err = clientset.AppsV1().Deployments(deployment.Namespace).Apply(
        context.TODO(),
        unstructured,
        metav1.ApplyOptions{
            FieldManager: "example-controller",
            Force:        true,
        },
    )
    return err
}
```

### Implementing the Scale Subresource

The Scale subresource allows you to scale resources without updating the entire resource:

```go
func scaleDeployment(clientset *kubernetes.Clientset, namespace, name string, replicas int32) error {
    // Get the current scale
    scale, err := clientset.AppsV1().Deployments(namespace).GetScale(context.TODO(), name, metav1.GetOptions{})
    if err != nil {
        return err
    }
    
    // Update the replicas
    scale.Spec.Replicas = replicas
    
    // Apply the scale
    _, err = clientset.AppsV1().Deployments(namespace).UpdateScale(context.TODO(), name, scale, metav1.UpdateOptions{})
    return err
}
```

For custom resources, you can implement the Scale subresource by adding the appropriate kubebuilder markers:

```go
//+kubebuilder:subresource:scale:specpath=.spec.replicas,statuspath=.status.replicas,selectorpath=.status.selector
```

## Best Practices and Common Patterns

### Idempotency

Ensure your controllers are idempotent by:
- Making reconciliation logic repeatable
- Using CreateOrUpdate patterns
- Avoiding assumptions about resource state

```go
// CreateOrUpdate pattern
func createOrUpdateDeployment(c client.Client, deployment *appsv1.Deployment) error {
    _, err := ctrl.CreateOrUpdate(context.TODO(), c, deployment, func() error {
        // Update deployment fields here
        return nil
    })
    return err
}
```

### Rate Limiting and Backoff

Use rate limiting to avoid overwhelming the API server:

```go
import "k8s.io/client-go/util/workqueue"

// Create a rate-limited workqueue
queue := workqueue.NewRateLimitingQueue(workqueue.DefaultControllerRateLimiter())

// Add reconcile requests to the queue
queue.Add(reconcile.Request{NamespacedName: types.NamespacedName{Name: "example", Namespace: "default"}})
```

### Optimistic Concurrency

Handle resource conflicts using the optimistic concurrency model:

```go
func updateWithRetry(c client.Client, obj client.Object) error {
    key := client.ObjectKeyFromObject(obj)
    return retry.RetryOnConflict(retry.DefaultRetry, func() error {
        // Fetch the latest version
        if err := c.Get(context.TODO(), key, obj); err != nil {
            return err
        }
        
        // Make your changes here
        // ...
        
        // Update the resource
        return c.Update(context.TODO(), obj)
    })
}
```

## Summary

In this chapter, we've explored:

1. Working with the Kubernetes API using client-go
2. CRUD operations on Kubernetes resources
3. Owner references and garbage collection
4. Watches and informers for event-driven controllers
5. Working with custom resources
6. Advanced patterns like server-side apply and optimistic concurrency

Understanding these concepts is essential for building robust Kubernetes operators. In the next chapter, we'll explore testing strategies for your operators.

## Exercises

1. Implement a controller that watches ConfigMaps and updates Deployments when their data changes
2. Create a utility function to handle optimistic concurrency with exponential backoff
3. Extend the WebService operator to use server-side apply
4. Implement the Scale subresource for the WebService CRD
5. Create a dynamic client to interact with WebService resources