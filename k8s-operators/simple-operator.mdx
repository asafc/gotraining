---
title: "Building a Simple Operator"
---

Now that we understand the fundamentals of Kubernetes controllers and Custom Resource Definitions, let's put this knowledge into practice by building a simple operator. In this chapter, we'll walk through implementing a basic operator that manages a custom resource, handling its lifecycle events.

## Objective

We'll build a "WebService" operator that manages simple web applications. Our operator will:
1. Create a Deployment for running the application
2. Create a Service to expose the application
3. Ensure the desired state is maintained
4. Handle updates to replicas and image versions
5. Clean up resources when a WebService is deleted

## Prerequisites

Before starting, ensure you have:
- A Kubernetes cluster (kind, minikube, or remote cluster)
- kubectl configured to work with your cluster
- Go 1.16+ installed
- operator-sdk installed
- kubebuilder or controller-runtime

## Step 1: Scaffold the Operator Project

Let's start by creating a new operator project using operator-sdk:

```bash
# Create a new directory for our project
mkdir webservice-operator
cd webservice-operator

# Initialize a new operator project
operator-sdk init --domain example.com --repo github.com/example/webservice-operator
```

## Step 2: Create the Custom Resource Definition

Next, we'll create a CRD for our WebService resource:

```bash
# Generate API for WebService CRD
operator-sdk create api --group apps --version v1alpha1 --kind WebService --resource --controller
```

## Step 3: Define the WebService API

Now let's modify the generated API to define our WebService resource. Open the file `api/v1alpha1/webservice_types.go` and update it:

```go
package v1alpha1

import (
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

// WebServiceSpec defines the desired state of WebService
type WebServiceSpec struct {
	// Image is the container image to use for the web service
	Image string `json:"image"`

	// Port is the container port to expose
	Port int32 `json:"port"`

	// Replicas is the number of pod replicas to run
	// +kubebuilder:default=1
	Replicas int32 `json:"replicas,omitempty"`
}

// WebServiceStatus defines the observed state of WebService
type WebServiceStatus struct {
	// AvailableReplicas is the number of replicas that are available
	AvailableReplicas int32 `json:"availableReplicas"`

	// Conditions represent the latest available observations of the WebService state
	Conditions []metav1.Condition `json:"conditions,omitempty"`
}

//+kubebuilder:object:root=true
//+kubebuilder:subresource:status
//+kubebuilder:printcolumn:name="Replicas",type="integer",JSONPath=".spec.replicas"
//+kubebuilder:printcolumn:name="Available",type="integer",JSONPath=".status.availableReplicas"
//+kubebuilder:printcolumn:name="Age",type="date",JSONPath=".metadata.creationTimestamp"

// WebService is the Schema for the webservices API
type WebService struct {
	metav1.TypeMeta   `json:",inline"`
	metav1.ObjectMeta `json:"metadata,omitempty"`

	Spec   WebServiceSpec   `json:"spec,omitempty"`
	Status WebServiceStatus `json:"status,omitempty"`
}

//+kubebuilder:object:root=true

// WebServiceList contains a list of WebService
type WebServiceList struct {
	metav1.TypeMeta `json:",inline"`
	metav1.ListMeta `json:"metadata,omitempty"`
	Items           []WebService `json:"items"`
}

func init() {
	SchemeBuilder.Register(&WebService{}, &WebServiceList{})
}
```

After updating the type, generate the code:

```bash
make generate
make manifests
```

## Step 4: Implement the Reconciliation Loop

Now let's implement the reconciliation logic in the controller. Open the file `controllers/webservice_controller.go` and update it:

```go
package controllers

import (
	"context"
	"fmt"
	"time"

	appsv1 "k8s.io/api/apps/v1"
	corev1 "k8s.io/api/core/v1"
	apierrors "k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/util/intstr"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/controller/controllerutil"
	"sigs.k8s.io/controller-runtime/pkg/log"

	appsv1alpha1 "github.com/example/webservice-operator/api/v1alpha1"
)

// WebServiceReconciler reconciles a WebService object
type WebServiceReconciler struct {
	client.Client
	Scheme *runtime.Scheme
}

//+kubebuilder:rbac:groups=apps.example.com,resources=webservices,verbs=get;list;watch;create;update;patch;delete
//+kubebuilder:rbac:groups=apps.example.com,resources=webservices/status,verbs=get;update;patch
//+kubebuilder:rbac:groups=apps.example.com,resources=webservices/finalizers,verbs=update
//+kubebuilder:rbac:groups=apps,resources=deployments,verbs=get;list;watch;create;update;patch;delete
//+kubebuilder:rbac:groups=core,resources=services,verbs=get;list;watch;create;update;patch;delete
//+kubebuilder:rbac:groups=core,resources=pods,verbs=get;list;watch

// Reconcile handles the reconciliation loop for WebService resources
func (r *WebServiceReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
	log := log.FromContext(ctx)
	log.Info("Reconciling WebService", "namespace", req.Namespace, "name", req.Name)

	// Fetch the WebService instance
	webService := &appsv1alpha1.WebService{}
	err := r.Get(ctx, req.NamespacedName, webService)
	if err != nil {
		if apierrors.IsNotFound(err) {
			// Request object not found, could have been deleted after reconcile request.
			log.Info("WebService resource not found. Ignoring since object must be deleted")
			return ctrl.Result{}, nil
		}
		// Error reading the object - requeue the request.
		log.Error(err, "Failed to get WebService")
		return ctrl.Result{}, err
	}

	// Check if the WebService instance is marked to be deleted
	// If it is, handle any cleanup operations (finalizers)
	if webService.GetDeletionTimestamp() != nil {
		// Resource is being deleted
		return ctrl.Result{}, nil
	}

	// Create or update Deployment
	deployment, err := r.reconcileDeployment(ctx, webService)
	if err != nil {
		log.Error(err, "Failed to reconcile Deployment")
		return ctrl.Result{RequeueAfter: time.Second * 10}, err
	}

	// Create or update Service
	service, err := r.reconcileService(ctx, webService)
	if err != nil {
		log.Error(err, "Failed to reconcile Service")
		return ctrl.Result{RequeueAfter: time.Second * 10}, err
	}

	// Update WebService status
	if err := r.updateStatus(ctx, webService, deployment); err != nil {
		log.Error(err, "Failed to update WebService status")
		return ctrl.Result{RequeueAfter: time.Second * 10}, err
	}

	log.Info("Successfully reconciled WebService", "namespace", req.Namespace, "name", req.Name)
	return ctrl.Result{}, nil
}

// reconcileDeployment ensures the Deployment for the WebService exists
func (r *WebServiceReconciler) reconcileDeployment(ctx context.Context, webService *appsv1alpha1.WebService) (*appsv1.Deployment, error) {
	// Define the Deployment
	deployment := &appsv1.Deployment{
		ObjectMeta: metav1.ObjectMeta{
			Name:      webService.Name,
			Namespace: webService.Namespace,
		},
	}

	// Set WebService instance as the owner of the Deployment
	if err := controllerutil.SetControllerReference(webService, deployment, r.Scheme); err != nil {
		return nil, err
	}

	// Create or update the Deployment
	op, err := ctrl.CreateOrUpdate(ctx, r.Client, deployment, func() error {
		// Set the deployment spec
		replicas := webService.Spec.Replicas
		if replicas == 0 {
			replicas = 1
		}

		deployment.Spec.Replicas = &replicas
		deployment.Spec.Selector = &metav1.LabelSelector{
			MatchLabels: map[string]string{
				"app": webService.Name,
			},
		}
		deployment.Spec.Template = corev1.PodTemplateSpec{
			ObjectMeta: metav1.ObjectMeta{
				Labels: map[string]string{
					"app": webService.Name,
				},
			},
			Spec: corev1.PodSpec{
				Containers: []corev1.Container{
					{
						Name:  "webservice",
						Image: webService.Spec.Image,
						Ports: []corev1.ContainerPort{
							{
								ContainerPort: webService.Spec.Port,
							},
						},
					},
				},
			},
		}
		return nil
	})

	if err != nil {
		return nil, err
	}

	log := log.FromContext(ctx)
	log.Info("Deployment reconciled", "operation", op)
	return deployment, nil
}

// reconcileService ensures the Service for the WebService exists
func (r *WebServiceReconciler) reconcileService(ctx context.Context, webService *appsv1alpha1.WebService) (*corev1.Service, error) {
	// Define the Service
	service := &corev1.Service{
		ObjectMeta: metav1.ObjectMeta{
			Name:      webService.Name,
			Namespace: webService.Namespace,
		},
	}

	// Set WebService instance as the owner of the Service
	if err := controllerutil.SetControllerReference(webService, service, r.Scheme); err != nil {
		return nil, err
	}

	// Create or update the Service
	op, err := ctrl.CreateOrUpdate(ctx, r.Client, service, func() error {
		// Set the service spec
		service.Spec.Selector = map[string]string{
			"app": webService.Name,
		}
		service.Spec.Ports = []corev1.ServicePort{
			{
				Port:       webService.Spec.Port,
				TargetPort: intstr.FromInt(int(webService.Spec.Port)),
			},
		}
		service.Spec.Type = corev1.ServiceTypeClusterIP
		return nil
	})

	if err != nil {
		return nil, err
	}

	log := log.FromContext(ctx)
	log.Info("Service reconciled", "operation", op)
	return service, nil
}

// updateStatus updates the status of the WebService resource
func (r *WebServiceReconciler) updateStatus(ctx context.Context, webService *appsv1alpha1.WebService, deployment *appsv1.Deployment) error {
	// Update the status with the current number of available replicas
	webService.Status.AvailableReplicas = deployment.Status.AvailableReplicas

	// Update conditions
	meta.SetStatusCondition(&webService.Status.Conditions, metav1.Condition{
		Type:               "Available",
		Status:             metav1.ConditionTrue,
		Reason:             "DeploymentAvailable",
		Message:            fmt.Sprintf("Deployment has %d available replicas", deployment.Status.AvailableReplicas),
		LastTransitionTime: metav1.Now(),
	})

	// Update the status
	return r.Status().Update(ctx, webService)
}

// SetupWithManager sets up the controller with the Manager.
func (r *WebServiceReconciler) SetupWithManager(mgr ctrl.Manager) error {
	return ctrl.NewControllerManagedBy(mgr).
		For(&appsv1alpha1.WebService{}).
		Owns(&appsv1.Deployment{}).
		Owns(&corev1.Service{}).
		Complete(r)
}
```

## Step 5: Build and Deploy the Operator

Let's build and deploy our operator to the Kubernetes cluster:

```bash
# Install the CRD into the cluster
make install

# Build and push the operator image
# Note: You would need to update the IMG variable with your container registry
make docker-build docker-push IMG=<your-registry>/webservice-operator:v0.1.0

# Deploy the operator in the cluster
make deploy IMG=<your-registry>/webservice-operator:v0.1.0
```

## Step 6: Test the Operator

Now let's test our operator by creating a WebService resource:

```yaml
apiVersion: apps.example.com/v1alpha1
kind: WebService
metadata:
  name: example-webservice
spec:
  image: nginx:1.19
  port: 80
  replicas: 2
```

Save this YAML to a file named `example-webservice.yaml` and apply it:

```bash
kubectl apply -f example-webservice.yaml
```

Verify that the resources are created:

```bash
# Check the WebService resource status
kubectl get webservice

# Check the deployment and pods
kubectl get deployment,pods

# Check the service 
kubectl get service
```

You should see a deployment with two replicas and a service that exposes the application.

## Step 7: Test Updates

Now let's update our WebService resource to test the reconciliation:

```yaml
apiVersion: apps.example.com/v1alpha1
kind: WebService
metadata:
  name: example-webservice
spec:
  image: nginx:1.20
  port: 80
  replicas: 3
```

Save this to a file named `example-webservice-updated.yaml` and apply it:

```bash
kubectl apply -f example-webservice-updated.yaml
```

Verify that the deployment is updated with the new image and replica count.

## Error Handling and Retries

In our operator, we've implemented some basic error handling:

1. We check if the resource exists before attempting reconciliation
2. We return appropriate errors to trigger retries
3. We use `RequeueAfter` to schedule reconciliations after a delay when errors occur

For more robust error handling, you might consider:

- Adding exponential backoff for retries
- Implementing a circuit breaker pattern
- Using finalizers to ensure cleanup of external resources
- Adding more specific error conditions to the status

## Reconciliation Loop Best Practices

When building operators, follow these best practices:

1. **Idempotency**: Your reconciliation logic should be idempotent, meaning it can be run multiple times without causing unintended side effects
2. **Defensive Programming**: Always validate inputs and handle edge cases
3. **Minimal Scope**: Only reconcile resources that your operator owns
4. **Logging**: Log important events and state changes
5. **Status Updates**: Keep the status subresource updated with the actual state

## Summary

In this chapter, we've built a simple operator that manages WebService resources. We've:

1. Scaffolded an operator project
2. Defined a custom resource (WebService)
3. Implemented a reconciliation loop
4. Deployed and tested the operator
5. Handled basic error conditions

The WebService operator demonstrated how to create and manage Kubernetes resources based on a custom resource specification. This pattern can be extended to manage more complex applications and infrastructure.

In the next chapter, we'll explore more advanced techniques for working with the Kubernetes API, including watches, informers, and more sophisticated client interactions.

## Homework

1. Add a health check to the WebService deployment
2. Implement a finalizer to handle resource deletion properly
3. Add support for environment variables in the WebService spec
4. Extend the operator to create an Ingress resource for the WebService
5. Add validation for the WebService image field